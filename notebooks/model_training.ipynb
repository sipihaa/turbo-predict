{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c499bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import git\n",
    "import mlflow\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eda345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Метрики и Scoring Functions ---\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Вычисляет расширенный набор метрик для RUL.\n",
    "    y_true, y_pred: torch.Tensor или numpy array\n",
    "    \"\"\"\n",
    "    if isinstance(y_true, torch.Tensor):\n",
    "        y_true = y_true.detach().cpu().numpy()\n",
    "    if isinstance(y_pred, torch.Tensor):\n",
    "        y_pred = y_pred.detach().cpu().numpy()\n",
    "        \n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "    \n",
    "    # Разница\n",
    "    d = y_pred - y_true\n",
    "    \n",
    "    # 1. MAE\n",
    "    mae = np.mean(np.abs(d))\n",
    "    \n",
    "    # 2. RMSE\n",
    "    rmse = np.sqrt(np.mean(d**2))\n",
    "    \n",
    "    # 3. MAPE (добавляем эпсилон для защиты от деления на 0)\n",
    "    epsilon = 1e-10\n",
    "    mape = np.mean(np.abs(d / (y_true + epsilon))) * 100\n",
    "    \n",
    "    # 4. PHM08 Score (NASA Scoring Function) [web:PHM08_Challenge]\n",
    "    # Функция асимметрична: ранние предсказания (d < 0) штрафуются меньше, чем поздние (d > 0)\n",
    "    # Формула: sum(exp(-d/13) - 1 если d < 0, иначе exp(d/10) - 1)\n",
    "    # *Внимание: в условии было указано score = sum(...) / n. Обычно в PHM08 используют просто sum,\n",
    "    # но для сопоставимости метрик лучше использовать среднее (mean) или следовать условию задачи.\n",
    "    # Здесь реализуем согласно вашему ТЗ: делим на n.\n",
    "    \n",
    "    n = len(d)\n",
    "    scores = np.where(d < 0, np.exp(-d/13) - 1, np.exp(d/10) - 1)\n",
    "    phm08_score = np.sum(scores) / n\n",
    "    \n",
    "    return {\"mae\": mae, \"rmse\": rmse, \"mape\": mape, \"phm08_score\": phm08_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5be19ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Адаптивная модель (Transfer Learning Ready) ---\n",
    "\n",
    "class AdaptiveLSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_1=64, hidden_dim_2=32, output_dim=1, dropout_prob=0.2):\n",
    "        super(AdaptiveLSTMModel, self).__init__()\n",
    "        \n",
    "        # === АДАПТИВНЫЙ ВХОДНОЙ СЛОЙ (ADAPTER) ===\n",
    "        # Проецируем входные признаки (любого кол-ва) в фиксированное скрытое пространство (hidden_dim_1).\n",
    "        # При Transfer Learning мы заменим только этот слой.\n",
    "        self.input_adapter = nn.Linear(input_dim, hidden_dim_1)\n",
    "        \n",
    "        # === BACKBONE (ЯДРО МОДЕЛИ) ===\n",
    "        # LSTM слои теперь принимают hidden_dim_1, а не input_dim.\n",
    "        # Это позволяет весам LSTM оставаться валидными даже если input_dim изменится.\n",
    "        self.backbone_lstm1 = nn.LSTM(hidden_dim_1, hidden_dim_1, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.backbone_lstm2 = nn.LSTM(hidden_dim_1, hidden_dim_2, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        # Финальная \"голова\"\n",
    "        self.fc = nn.Linear(hidden_dim_2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_dim)\n",
    "        \n",
    "        # 1. Адаптация входа\n",
    "        x_embedded = self.input_adapter(x) # -> (batch_size, seq_len, hidden_dim_1)\n",
    "        \n",
    "        # 2. Проход через Backbone\n",
    "        lstm1_out, _ = self.backbone_lstm1(x_embedded)\n",
    "        out = self.dropout1(lstm1_out)\n",
    "        \n",
    "        lstm2_out, _ = self.backbone_lstm2(out)\n",
    "        \n",
    "        # Берем выход последнего временного шага\n",
    "        last_hidden_state = lstm2_out[:, -1, :] \n",
    "        out = self.dropout2(last_hidden_state)\n",
    "        \n",
    "        # 3. Финальный прогноз\n",
    "        final_output = self.fc(out)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c430d1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Git Commit Hash: 3c536c259249c0e1f31ae47fa99a9189e8d28ea8\n"
     ]
    }
   ],
   "source": [
    "# Указываем MLflow, куда отправлять данные\n",
    "mlflow.set_tracking_uri(\"http://213.21.252.250:5000\")\n",
    "\n",
    "# Задаем имя эксперимента\n",
    "mlflow.set_experiment(\"LSTM_TransferLearning_Ready\")\n",
    "\n",
    "# --- Получаем хеш коммита Git ---\n",
    "try:\n",
    "    repo = git.Repo(search_parent_directories=True)\n",
    "    git_commit_hash = repo.head.object.hexsha\n",
    "except Exception as e:\n",
    "    git_commit_hash = \"N/A\" # На случай, если скрипт запущен не из Git-репозитория\n",
    "    print(f\"Warning: Could not get git commit hash. {e}\")\n",
    "\n",
    "print(f\"Current Git Commit Hash: {git_commit_hash}\")\n",
    "\n",
    "# --- Параметры, которые нужно логировать ---\n",
    "# Параметры из скрипта нарезки данных (sample_creator)\n",
    "data_params = {\n",
    "    \"window_size\": 50,\n",
    "    \"step\": 1,\n",
    "    \"sampling_rate\": 10\n",
    "}\n",
    "\n",
    "# Гиперпараметры модели\n",
    "model_params = {\n",
    "    \"epochs\": 7,\n",
    "    \"batch_size\": 128,\n",
    "    \"validation_split\": 0.2,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss\": \"mean_squared_error\",\n",
    "    \"lr\": 0.002,\n",
    "    \"hidden_dim_1\": 32,\n",
    "    \"hidden_dim_2\": 16\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c208c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_merge_data(npz_units):\n",
    "      sample_array_lst = []\n",
    "      label_array_lst = []\n",
    "      for npz_unit in npz_units:\n",
    "        loaded = np.load(npz_unit)\n",
    "        sample_array_lst.append(loaded['sample'])\n",
    "        label_array_lst.append(loaded['label'])\n",
    "      sample_array = np.dstack(sample_array_lst)\n",
    "      label_array = np.concatenate(label_array_lst)\n",
    "      sample_array = sample_array.transpose(2, 0, 1)\n",
    "      return sample_array, label_array\n",
    "\n",
    "processed_dir = '../data/processed/'\n",
    "\n",
    "# Собираем пути к файлам для train и test\n",
    "train_files = [os.path.join(processed_dir, f) for f in os.listdir(processed_dir) if f.startswith(('Unit2_', 'Unit5_', 'Unit10_', 'Unit16_', 'Unit18_', 'Unit20_'))]\n",
    "test_files = [os.path.join(processed_dir, f) for f in os.listdir(processed_dir) if f.startswith(('Unit11_', 'Unit14_', 'Unit15_'))]\n",
    "print(train_files)\n",
    "\n",
    "# Загружаем данные\n",
    "X_train, y_train = load_and_merge_data(train_files)\n",
    "X_test, y_test = load_and_merge_data(test_files)\n",
    "\n",
    "# Определяем форму входных данных из X_train\n",
    "n_timesteps, n_features = X_train.shape[1], X_train.shape[2]\n",
    "\n",
    "print('Размер обучающей выборки (X):', X_train.shape)\n",
    "print('Размер обучающей выборки (y):', y_train.shape)\n",
    "print('Размер тестовой выборки (X):', X_test.shape)\n",
    "print('Размер тестовой выборки (y):', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ab700d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MLflow run...\n",
      "Parameters logged.\n",
      "['../data/processed/Unit16_win50_str1_smp10.npz', '../data/processed/Unit5_win50_str1_smp10.npz', '../data/processed/Unit18_win50_str1_smp10.npz', '../data/processed/Unit20_win50_str1_smp10.npz', '../data/processed/Unit2_win50_str1_smp10.npz', '../data/processed/Unit10_win50_str1_smp10.npz']\n",
      "Размер обучающей выборки (X): (526051, 50, 20)\n",
      "Размер обучающей выборки (y): (526051,)\n",
      "Размер тестовой выборки (X): (125227, 50, 20)\n",
      "Размер тестовой выборки (y): (125227,)\n",
      "Epoch [1/20], rain oss: 829.8270, Val Loss: 505.0620, Val MAE: 19.1880\n",
      "Epoch [2/20], rain oss: 533.2666, Val Loss: 501.4155, Val MAE: 19.1737\n",
      "Epoch [3/20], rain oss: 529.3412, Val Loss: 501.5684, Val MAE: 19.1729\n",
      "Epoch [4/20], rain oss: 340.5360, Val Loss: 57.6770, Val MAE: 5.4372\n",
      "Epoch [5/20], rain oss: 89.3959, Val Loss: 55.7372, Val MAE: 5.5416\n",
      "Epoch [6/20], rain oss: 80.7433, Val Loss: 42.4201, Val MAE: 4.6042\n",
      "Epoch [7/20], rain oss: 75.7152, Val Loss: 42.3691, Val MAE: 4.6465\n",
      "Epoch [8/20], rain oss: 72.4590, Val Loss: 45.6029, Val MAE: 4.9609\n",
      "Epoch [9/20], rain oss: 69.7325, Val Loss: 50.3026, Val MAE: 5.2415\n",
      "Epoch [10/20], rain oss: 67.4636, Val Loss: 42.2230, Val MAE: 4.7861\n",
      "Epoch [11/20], rain oss: 66.1981, Val Loss: 67.8372, Val MAE: 6.6040\n",
      "Epoch [12/20], rain oss: 64.5392, Val Loss: 44.6050, Val MAE: 5.1415\n",
      "Epoch [13/20], rain oss: 62.9703, Val Loss: 44.1168, Val MAE: 5.0782\n",
      "Epoch [14/20], rain oss: 61.6939, Val Loss: 52.0719, Val MAE: 5.3869\n",
      "Epoch [15/20], rain oss: 60.3587, Val Loss: 53.1476, Val MAE: 5.5756\n",
      "Epoch [16/20], rain oss: 59.6904, Val Loss: 43.6463, Val MAE: 5.0822\n",
      "Epoch [17/20], rain oss: 59.4717, Val Loss: 47.7033, Val MAE: 5.3804\n",
      "Epoch [18/20], rain oss: 58.0993, Val Loss: 44.8553, Val MAE: 5.3345\n",
      "Epoch [19/20], rain oss: 58.2971, Val Loss: 62.4155, Val MAE: 6.3918\n",
      "Epoch [20/20], rain oss: 57.0383, Val Loss: 52.5547, Val MAE: 5.7381\n",
      "\n",
      "Test MAE: 7.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/28 13:01:22 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics logged: {'mae': 7.720066614368882}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/11/28 13:01:27 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    print(\"Starting MLflow run...\")\n",
    "\n",
    "    # --- Логируем параметры ---\n",
    "    mlflow.log_params(data_params)\n",
    "    mlflow.log_params(model_params)\n",
    "    mlflow.set_tag(\"git_commit\", git_commit_hash)\n",
    "    print(\"Parameters logged.\")\n",
    "\n",
    "    \n",
    "    # --- Определяем модель LSTM на PyTorch ---\n",
    "    class LSTMModel(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim_1, hidden_dim_2, output_dim=1, dropout_prob=0.2):\n",
    "            super(LSTMModel, self).__init__()\n",
    "            # Первый LSTM слой\n",
    "            self.lstm1 = nn.LSTM(input_dim, hidden_dim_1, batch_first=True)\n",
    "            # batch_first=True очень важен, чтобы входные данные имели формат (batch, seq, feature), как в Keras\n",
    "            \n",
    "            self.dropout1 = nn.Dropout(dropout_prob)\n",
    "            \n",
    "            # Второй LSTM слой\n",
    "            # Он принимает на вход скрытое состояние первого слоя (hidden_dim_1)\n",
    "            self.lstm2 = nn.LSTM(hidden_dim_1, hidden_dim_2, batch_first=True)\n",
    "            \n",
    "            self.dropout2 = nn.Dropout(dropout_prob)\n",
    "            \n",
    "            # Полносвязный слой для финального прогноза\n",
    "            self.fc = nn.Linear(hidden_dim_2, output_dim)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Первый LSTM слой\n",
    "            # LSTM возвращает output и кортеж (hidden_state, cell_state)\n",
    "            # Нам нужен output для следующего слоя\n",
    "            lstm1_out, _ = self.lstm1(x)\n",
    "            \n",
    "            # Dropout\n",
    "            out = self.dropout1(lstm1_out)\n",
    "            \n",
    "            # Второй LSTM слой\n",
    "            # Нам нужен только выход последнего временного шага\n",
    "            lstm2_out, _ = self.lstm2(out)\n",
    "            last_hidden_state = lstm2_out[:, -1, :] # Берем выход последнего элемента последовательности\n",
    "            \n",
    "            # Dropout\n",
    "            out = self.dropout2(last_hidden_state)\n",
    "            \n",
    "            # Полносвязный слой\n",
    "            final_output = self.fc(out)\n",
    "            return final_output\n",
    "\n",
    "    # --- Подготовка данных для PyTorch ---\n",
    "    # 1. Преобразуем numpy массивы в torch тензоры\n",
    "    X_train_tensor = torch.from_numpy(X_train).float()\n",
    "    y_train_tensor = torch.from_numpy(y_train).float().view(-1, 1) # Убедимся, что форма (batch_size, 1)\n",
    "    X_test_tensor = torch.from_numpy(X_test).float()\n",
    "    y_test_tensor = torch.from_numpy(y_test).float().view(-1, 1)\n",
    "\n",
    "    # 2. Создаем датасеты\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    # 3. Разделяем на обучающую и валидационную выборки вручную\n",
    "    val_split = model_params['validation_split']\n",
    "    dataset_size = len(train_dataset)\n",
    "    val_size = int(val_split * dataset_size)\n",
    "    train_size = dataset_size - val_size\n",
    "    train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "    # 4. Создаем загрузчики данных (DataLoader), которые будут подавать данные батчами\n",
    "    train_loader = DataLoader(dataset=train_subset, batch_size=model_params['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(dataset=val_subset, batch_size=model_params['batch_size'])\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=model_params['batch_size'])\n",
    "\n",
    "    # Инициализация модели\n",
    "    device = torch.device(\"cpu\") # Требование: работать на CPU\n",
    "    \n",
    "    model = AdaptiveLSTMModel(\n",
    "        input_dim=n_features,\n",
    "        hidden_dim_1=model_params[\"hidden_dim_1\"],\n",
    "        hidden_dim_2=model_params[\"hidden_dim_2\"],\n",
    "        dropout_prob=model_params[\"dropout\"]\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=model_params[\"lr\"])\n",
    "\n",
    "    # Цикл обучения\n",
    "    for epoch in range(model_params[\"epochs\"]):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        mlflow.log_metric(\"train_mse_loss\", avg_train_loss, step=epoch)\n",
    "\n",
    "        # Валидация\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_targets = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                val_preds.append(outputs.numpy())\n",
    "                val_targets.append(labels.numpy())\n",
    "        \n",
    "        val_preds = np.concatenate(val_preds)\n",
    "        val_targets = np.concatenate(val_targets)\n",
    "        \n",
    "        # Расчет всех метрик\n",
    "        val_metrics = calculate_metrics(val_targets, val_preds)\n",
    "        \n",
    "        # Логирование метрик валидации\n",
    "        for name, value in val_metrics.items():\n",
    "            mlflow.log_metric(f\"val_{name}\", value, step=epoch)\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}/{model_params['epochs']} | Train Loss: {avg_train_loss:.2f} | Val MAE: {val_metrics['mae']:.2f} | PHM08: {val_metrics['phm08_score']:.2f}\")\n",
    "\n",
    "    # --- 5. Финальный тест и сохранение ---\n",
    "    print(\"\\nEvaluating on Test Set...\")\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    test_targets = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            test_preds.append(outputs.numpy())\n",
    "            test_targets.append(labels.numpy())\n",
    "    \n",
    "    test_preds = np.concatenate(test_preds)\n",
    "    test_targets = np.concatenate(test_targets)\n",
    "    \n",
    "    test_metrics = calculate_metrics(test_targets, test_preds)\n",
    "    print(f\"Test Metrics: {test_metrics}\")\n",
    "    \n",
    "    # Логируем финальные метрики с префиксом test_\n",
    "    for name, value in test_metrics.items():\n",
    "        mlflow.log_metric(f\"test_{name}\", value)\n",
    "\n",
    "    # 1. Сохранение ПОЛНОЙ модели\n",
    "    mlflow.pytorch.log_model(model, \"full_model\")\n",
    "    \n",
    "    # 2. Сохранение BACKBONE (State Dict без входного слоя) для Transfer Learning\n",
    "    # Исключаем веса input_adapter, чтобы клиент мог инициализировать свои\n",
    "    backbone_state_dict = {k: v for k, v in model.state_dict().items() if \"input_adapter\" not in k}\n",
    "    torch.save(backbone_state_dict, \"backbone.pth\")\n",
    "    mlflow.log_artifact(\"backbone.pth\", artifact_path=\"transfer_learning_artifacts\")\n",
    "    \n",
    "    print(\"Run Complete. Artifacts logged.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
