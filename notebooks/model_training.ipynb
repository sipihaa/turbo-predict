{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71c499bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import git\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c430d1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Git Commit Hash: a616b85e89fd06f5db64b37c1645ef18d8567947\n"
     ]
    }
   ],
   "source": [
    "# Ð£ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ MLflow, ÐºÑƒÐ´Ð° Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð»ÑÑ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ðµ\n",
    "mlflow.set_tracking_uri(\"http://213.21.252.250:5000\")\n",
    "\n",
    "# Ð—Ð°Ð´Ð°ÐµÐ¼ Ð¸Ð¼Ñ ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð°\n",
    "mlflow.set_experiment(\"LSTM (test)\")\n",
    "\n",
    "# --- ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ñ…ÐµÑˆ ÐºÐ¾Ð¼Ð¼Ð¸Ñ‚Ð° Git ---\n",
    "try:\n",
    "    repo = git.Repo(search_parent_directories=True)\n",
    "    git_commit_hash = repo.head.object.hexsha\n",
    "except Exception as e:\n",
    "    git_commit_hash = \"N/A\" # ÐÐ° ÑÐ»ÑƒÑ‡Ð°Ð¹, ÐµÑÐ»Ð¸ ÑÐºÑ€Ð¸Ð¿Ñ‚ Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½ Ð½Ðµ Ð¸Ð· Git-Ñ€ÐµÐ¿Ð¾Ð·Ð¸Ñ‚Ð¾Ñ€Ð¸Ñ\n",
    "    print(f\"Warning: Could not get git commit hash. {e}\")\n",
    "\n",
    "print(f\"Current Git Commit Hash: {git_commit_hash}\")\n",
    "\n",
    "# --- ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð½ÑƒÐ¶Ð½Ð¾ Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ ---\n",
    "# ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð¸Ð· ÑÐºÑ€Ð¸Ð¿Ñ‚Ð° Ð½Ð°Ñ€ÐµÐ·ÐºÐ¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… (sample_creator)\n",
    "data_params = {\n",
    "    \"window_size\": 50,\n",
    "    \"step\": 1,\n",
    "    \"sampling_rate\": 10\n",
    "}\n",
    "\n",
    "# Ð“Ð¸Ð¿ÐµÑ€Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð¼Ð¾Ð´ÐµÐ»Ð¸\n",
    "model_params = {\n",
    "    \"epochs\": 5,\n",
    "    \"batch_size\": 128,\n",
    "    \"validation_split\": 0.2,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss\": \"mean_squared_error\",\n",
    "    \"lr\": 0.001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30ab700d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MLflow run...\n",
      "Parameters logged.\n",
      "['../data/processed/Unit16_win50_str1_smp10.npz', '../data/processed/Unit5_win50_str1_smp10.npz', '../data/processed/Unit18_win50_str1_smp10.npz', '../data/processed/Unit20_win50_str1_smp10.npz', '../data/processed/Unit2_win50_str1_smp10.npz', '../data/processed/Unit10_win50_str1_smp10.npz']\n",
      "ðŸƒ View run glamorous-steed-925 at: http://213.21.252.250:5000/#/experiments/1/runs/49647fe4ac4645a9871db8d7bf6781ab\n",
      "ðŸ§ª View experiment at: http://213.21.252.250:5000/#/experiments/1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(train_files)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m X_train, y_train = \u001b[43mload_and_merge_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m X_test, y_test = load_and_merge_data(test_files)\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mÐ Ð°Ð·Ð¼ÐµÑ€ Ð¾Ð±ÑƒÑ‡Ð°ÑŽÑ‰ÐµÐ¹ Ð²Ñ‹Ð±Ð¾Ñ€ÐºÐ¸ (X):\u001b[39m\u001b[33m'\u001b[39m, X_train.shape)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mload_and_merge_data\u001b[39m\u001b[34m(npz_units)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m npz_unit \u001b[38;5;129;01min\u001b[39;00m npz_units:\n\u001b[32m     14\u001b[39m   loaded = np.load(npz_unit)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m   sample_array_lst.append(\u001b[43mloaded\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msample\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m     16\u001b[39m   label_array_lst.append(loaded[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     17\u001b[39m sample_array = np.dstack(sample_array_lst)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/turbo-predict/.venv/lib/python3.12/site-packages/numpy/lib/npyio.py:256\u001b[39m, in \u001b[36mNpzFile.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m magic == \u001b[38;5;28mformat\u001b[39m.MAGIC_PREFIX:\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mbytes\u001b[39m = \u001b[38;5;28mself\u001b[39m.zip.open(key)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.zip.read(key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/turbo-predict/.venv/lib/python3.12/site-packages/numpy/lib/format.py:831\u001b[39m, in \u001b[36mread_array\u001b[39m\u001b[34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[39m\n\u001b[32m    829\u001b[39m             read_count = \u001b[38;5;28mmin\u001b[39m(max_read_count, count - i)\n\u001b[32m    830\u001b[39m             read_size = \u001b[38;5;28mint\u001b[39m(read_count * dtype.itemsize)\n\u001b[32m--> \u001b[39m\u001b[32m831\u001b[39m             data = \u001b[43m_read_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43marray data\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    832\u001b[39m             array[i:i+read_count] = numpy.frombuffer(data, dtype=dtype,\n\u001b[32m    833\u001b[39m                                                      count=read_count)\n\u001b[32m    835\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fortran_order:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/turbo-predict/.venv/lib/python3.12/site-packages/numpy/lib/format.py:966\u001b[39m, in \u001b[36m_read_bytes\u001b[39m\u001b[34m(fp, size, error_template)\u001b[39m\n\u001b[32m    961\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    962\u001b[39m     \u001b[38;5;66;03m# io files (default in python3) return None or raise on\u001b[39;00m\n\u001b[32m    963\u001b[39m     \u001b[38;5;66;03m# would-block, python2 file will truncate, probably nothing can be\u001b[39;00m\n\u001b[32m    964\u001b[39m     \u001b[38;5;66;03m# done about that.  note that regular files can't be non-blocking\u001b[39;00m\n\u001b[32m    965\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m966\u001b[39m         r = \u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    967\u001b[39m         data += r\n\u001b[32m    968\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(r) == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) == size:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/zipfile/__init__.py:981\u001b[39m, in \u001b[36mZipExtFile.read\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m    979\u001b[39m \u001b[38;5;28mself\u001b[39m._offset = \u001b[32m0\u001b[39m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m n > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._eof:\n\u001b[32m--> \u001b[39m\u001b[32m981\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n < \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[32m    983\u001b[39m         \u001b[38;5;28mself\u001b[39m._readbuffer = data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/zipfile/__init__.py:1057\u001b[39m, in \u001b[36mZipExtFile._read1\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m   1055\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compress_type == ZIP_DEFLATED:\n\u001b[32m   1056\u001b[39m     n = \u001b[38;5;28mmax\u001b[39m(n, \u001b[38;5;28mself\u001b[39m.MIN_READ_SIZE)\n\u001b[32m-> \u001b[39m\u001b[32m1057\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decompressor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1058\u001b[39m     \u001b[38;5;28mself\u001b[39m._eof = (\u001b[38;5;28mself\u001b[39m._decompressor.eof \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1059\u001b[39m                  \u001b[38;5;28mself\u001b[39m._compress_left <= \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m   1060\u001b[39m                  \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decompressor.unconsumed_tail)\n\u001b[32m   1061\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._eof:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    print(\"Starting MLflow run...\")\n",
    "\n",
    "    # --- Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ ---\n",
    "    mlflow.log_params(data_params)\n",
    "    mlflow.log_params(model_params)\n",
    "    mlflow.set_tag(\"git_commit\", git_commit_hash)\n",
    "    print(\"Parameters logged.\")\n",
    "\n",
    "    def load_and_merge_data(npz_units):\n",
    "      sample_array_lst = []\n",
    "      label_array_lst = []\n",
    "      for npz_unit in npz_units:\n",
    "        loaded = np.load(npz_unit)\n",
    "        sample_array_lst.append(loaded['sample'])\n",
    "        label_array_lst.append(loaded['label'])\n",
    "      sample_array = np.dstack(sample_array_lst)\n",
    "      label_array = np.concatenate(label_array_lst)\n",
    "      sample_array = sample_array.transpose(2, 0, 1)\n",
    "      return sample_array, label_array\n",
    "\n",
    "    processed_dir = '../data/processed/'\n",
    "\n",
    "    # Ð¡Ð¾Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð¿ÑƒÑ‚Ð¸ Ðº Ñ„Ð°Ð¹Ð»Ð°Ð¼ Ð´Ð»Ñ train Ð¸ test\n",
    "    train_files = [os.path.join(processed_dir, f) for f in os.listdir(processed_dir) if f.startswith(('Unit2_', 'Unit5_', 'Unit10_', 'Unit16_', 'Unit18_', 'Unit20_'))]\n",
    "    test_files = [os.path.join(processed_dir, f) for f in os.listdir(processed_dir) if f.startswith(('Unit11_', 'Unit14_', 'Unit15_'))]\n",
    "    print(train_files)\n",
    "\n",
    "    # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ\n",
    "    X_train, y_train = load_and_merge_data(train_files)\n",
    "    X_test, y_test = load_and_merge_data(test_files)\n",
    "\n",
    "    print('Ð Ð°Ð·Ð¼ÐµÑ€ Ð¾Ð±ÑƒÑ‡Ð°ÑŽÑ‰ÐµÐ¹ Ð²Ñ‹Ð±Ð¾Ñ€ÐºÐ¸ (X):', X_train.shape)\n",
    "    print('Ð Ð°Ð·Ð¼ÐµÑ€ Ð¾Ð±ÑƒÑ‡Ð°ÑŽÑ‰ÐµÐ¹ Ð²Ñ‹Ð±Ð¾Ñ€ÐºÐ¸ (y):', y_train.shape)\n",
    "    print('Ð Ð°Ð·Ð¼ÐµÑ€ Ñ‚ÐµÑÑ‚Ð¾Ð²Ð¾Ð¹ Ð²Ñ‹Ð±Ð¾Ñ€ÐºÐ¸ (X):', X_test.shape)\n",
    "    print('Ð Ð°Ð·Ð¼ÐµÑ€ Ñ‚ÐµÑÑ‚Ð¾Ð²Ð¾Ð¹ Ð²Ñ‹Ð±Ð¾Ñ€ÐºÐ¸ (y):', y_test.shape)\n",
    "\n",
    "    # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ñ„Ð¾Ñ€Ð¼Ñƒ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð· X_train\n",
    "    n_timesteps, n_features = X_train.shape[1], X_train.shape[2]\n",
    "\n",
    "    # --- ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ LSTM Ð½Ð° PyTorch ---\n",
    "    class LSTMModel(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim_1, hidden_dim_2, output_dim=1, dropout_prob=0.2):\n",
    "            super(LSTMModel, self).__init__()\n",
    "            # ÐŸÐµÑ€Ð²Ñ‹Ð¹ LSTM ÑÐ»Ð¾Ð¹\n",
    "            self.lstm1 = nn.LSTM(input_dim, hidden_dim_1, batch_first=True)\n",
    "            # batch_first=True Ð¾Ñ‡ÐµÐ½ÑŒ Ð²Ð°Ð¶ÐµÐ½, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð¼ÐµÐ»Ð¸ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ (batch, seq, feature), ÐºÐ°Ðº Ð² Keras\n",
    "            \n",
    "            self.dropout1 = nn.Dropout(dropout_prob)\n",
    "            \n",
    "            # Ð’Ñ‚Ð¾Ñ€Ð¾Ð¹ LSTM ÑÐ»Ð¾Ð¹\n",
    "            # ÐžÐ½ Ð¿Ñ€Ð¸Ð½Ð¸Ð¼Ð°ÐµÑ‚ Ð½Ð° Ð²Ñ…Ð¾Ð´ ÑÐºÑ€Ñ‹Ñ‚Ð¾Ðµ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ Ð¿ÐµÑ€Ð²Ð¾Ð³Ð¾ ÑÐ»Ð¾Ñ (hidden_dim_1)\n",
    "            self.lstm2 = nn.LSTM(hidden_dim_1, hidden_dim_2, batch_first=True)\n",
    "            \n",
    "            self.dropout2 = nn.Dropout(dropout_prob)\n",
    "            \n",
    "            # ÐŸÐ¾Ð»Ð½Ð¾ÑÐ²ÑÐ·Ð½Ñ‹Ð¹ ÑÐ»Ð¾Ð¹ Ð´Ð»Ñ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·Ð°\n",
    "            self.fc = nn.Linear(hidden_dim_2, output_dim)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # ÐŸÐµÑ€Ð²Ñ‹Ð¹ LSTM ÑÐ»Ð¾Ð¹\n",
    "            # LSTM Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ output Ð¸ ÐºÐ¾Ñ€Ñ‚ÐµÐ¶ (hidden_state, cell_state)\n",
    "            # ÐÐ°Ð¼ Ð½ÑƒÐ¶ÐµÐ½ output Ð´Ð»Ñ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÐµÐ³Ð¾ ÑÐ»Ð¾Ñ\n",
    "            lstm1_out, _ = self.lstm1(x)\n",
    "            \n",
    "            # Dropout\n",
    "            out = self.dropout1(lstm1_out)\n",
    "            \n",
    "            # Ð’Ñ‚Ð¾Ñ€Ð¾Ð¹ LSTM ÑÐ»Ð¾Ð¹\n",
    "            # ÐÐ°Ð¼ Ð½ÑƒÐ¶ÐµÐ½ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð²Ñ‹Ñ…Ð¾Ð´ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐ³Ð¾ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð³Ð¾ ÑˆÐ°Ð³Ð°\n",
    "            lstm2_out, _ = self.lstm2(out)\n",
    "            last_hidden_state = lstm2_out[:, -1, :] # Ð‘ÐµÑ€ÐµÐ¼ Ð²Ñ‹Ñ…Ð¾Ð´ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐ³Ð¾ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð° Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸\n",
    "            \n",
    "            # Dropout\n",
    "            out = self.dropout2(last_hidden_state)\n",
    "            \n",
    "            # ÐŸÐ¾Ð»Ð½Ð¾ÑÐ²ÑÐ·Ð½Ñ‹Ð¹ ÑÐ»Ð¾Ð¹\n",
    "            final_output = self.fc(out)\n",
    "            return final_output\n",
    "\n",
    "    # --- ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ PyTorch ---\n",
    "    # 1. ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ numpy Ð¼Ð°ÑÑÐ¸Ð²Ñ‹ Ð² torch Ñ‚ÐµÐ½Ð·Ð¾Ñ€Ñ‹\n",
    "    X_train_tensor = torch.from_numpy(X_train).float()\n",
    "    y_train_tensor = torch.from_numpy(y_train).float().view(-1, 1) # Ð£Ð±ÐµÐ´Ð¸Ð¼ÑÑ, Ñ‡Ñ‚Ð¾ Ñ„Ð¾Ñ€Ð¼Ð° (batch_size, 1)\n",
    "    X_test_tensor = torch.from_numpy(X_test).float()\n",
    "    y_test_tensor = torch.from_numpy(y_test).float().view(-1, 1)\n",
    "\n",
    "    # 2. Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ñ‹\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    # 3. Ð Ð°Ð·Ð´ÐµÐ»ÑÐµÐ¼ Ð½Ð° Ð¾Ð±ÑƒÑ‡Ð°ÑŽÑ‰ÑƒÑŽ Ð¸ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¾Ð½Ð½ÑƒÑŽ Ð²Ñ‹Ð±Ð¾Ñ€ÐºÐ¸ Ð²Ñ€ÑƒÑ‡Ð½ÑƒÑŽ\n",
    "    val_split = model_params['validation_split']\n",
    "    dataset_size = len(train_dataset)\n",
    "    val_size = int(val_split * dataset_size)\n",
    "    train_size = dataset_size - val_size\n",
    "    train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "    # 4. Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð·Ð°Ð³Ñ€ÑƒÐ·Ñ‡Ð¸ÐºÐ¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… (DataLoader), ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð±ÑƒÐ´ÑƒÑ‚ Ð¿Ð¾Ð´Ð°Ð²Ð°Ñ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð±Ð°Ñ‚Ñ‡Ð°Ð¼Ð¸\n",
    "    train_loader = DataLoader(dataset=train_subset, batch_size=model_params['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(dataset=val_subset, batch_size=model_params['batch_size'])\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=model_params['batch_size'])\n",
    "\n",
    "    # --- Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸, Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð° Ð¸ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ Ð¿Ð¾Ñ‚ÐµÑ€ÑŒ ---\n",
    "    model = LSTMModel(input_dim=n_features, hidden_dim_1=16, hidden_dim_2=8)\n",
    "\n",
    "    # Ð’Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ñ„ÑƒÐ½ÐºÑ†Ð¸ÑŽ Ð¿Ð¾Ñ‚ÐµÑ€ÑŒ (Ð°Ð½Ð°Ð»Ð¾Ð³ 'loss' Ð² Keras)\n",
    "    if model_params['loss'] == 'mse':\n",
    "        criterion = nn.MSELoss()\n",
    "    else:\n",
    "        # Ð¼Ð¾Ð¶Ð½Ð¾ Ð´Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ Ð´Ñ€ÑƒÐ³Ð¸Ðµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ Ð¿Ð¾Ñ‚ÐµÑ€ÑŒ\n",
    "        criterion = nn.MSELoss() \n",
    "\n",
    "    # Ð’Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ (Ð°Ð½Ð°Ð»Ð¾Ð³ 'optimizer' Ð² Keras)\n",
    "    if model_params['optimizer'] == 'adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=model_params['lr'])\n",
    "    else:\n",
    "        # Ð¼Ð¾Ð¶Ð½Ð¾ Ð´Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ Ð´Ñ€ÑƒÐ³Ð¸Ðµ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ñ‹\n",
    "        optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    # --- Ð¦Ð¸ÐºÐ» Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ ---\n",
    "    for epoch in range(model_params['epochs']):\n",
    "        model.train() # ÐŸÐµÑ€ÐµÐ²Ð¾Ð´Ð¸Ð¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð² Ñ€ÐµÐ¶Ð¸Ð¼ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            # 1. ÐžÐ±Ð½ÑƒÐ»ÑÐµÐ¼ Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ñ‹\n",
    "            optimizer.zero_grad()\n",
    "            # 2. ÐŸÑ€ÑÐ¼Ð¾Ð¹ Ð¿Ñ€Ð¾Ñ…Ð¾Ð´ (forward pass)\n",
    "            outputs = model(inputs)\n",
    "            # 3. Ð Ð°ÑÑ‡ÐµÑ‚ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ Ð¿Ð¾Ñ‚ÐµÑ€ÑŒ\n",
    "            loss = criterion(outputs, labels)\n",
    "            # 4. ÐžÐ±Ñ€Ð°Ñ‚Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ…Ð¾Ð´ (backward pass)\n",
    "            loss.backward()\n",
    "            # 5. Ð¨Ð°Ð³ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð°\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Ð Ð°ÑÑ‡ÐµÑ‚ Ð¸ Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ ÑÑ€ÐµÐ´Ð½ÐµÐ¹ Ð¿Ð¾Ñ‚ÐµÑ€Ð¸ Ð·Ð° ÑÐ¿Ð¾Ñ…Ñƒ\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
    "        # --- Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ð½Ð° ÐºÐ°Ð¶Ð´Ð¾Ð¹ ÑÐ¿Ð¾Ñ…Ðµ --\n",
    "        model.eval() # ÐŸÐµÑ€ÐµÐ²Ð¾Ð´Ð¸Ð¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð² Ñ€ÐµÐ¶Ð¸Ð¼ Ð¾Ñ†ÐµÐ½ÐºÐ¸\n",
    "        val_loss = 0.0\n",
    "        val_mae = 0.0\n",
    "        with torch.no_grad(): # ÐžÑ‚ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ Ñ€Ð°ÑÑ‡ÐµÑ‚ Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð¾Ð²\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                # Ð¡Ñ‡Ð¸Ñ‚Ð°ÐµÐ¼ MAE Ð²Ñ€ÑƒÑ‡Ð½Ñƒ\n",
    "                val_mae += torch.abs(outputs - labels).sum().item()\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_val_mae = val_mae / len(val_subset)\n",
    "        mlflow.log_metric(\"val_loss\", avg_val_loss, step=epoch)\n",
    "        mlflow.log_metric(\"val_mae\", avg_val_mae, step=epoch)\n",
    "        print(f\"Epoch [{epoch+1}/{model_params['epochs']}], rain oss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val MAE: {avg_val_mae:.4f}\")\n",
    "        # --- ÐžÑ†ÐµÐ½ÐºÐ° Ð½Ð° Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… ---\n",
    "        model.eval()\n",
    "        test_mae = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                test_mae += torch.abs(outputs - labels).sum().item()\n",
    "        \n",
    "        final_mae = test_mae / len(test_dataset)\n",
    "        print(f'\\nTest MAE: {final_mae:.2f}')\n",
    "        # Ð›Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¼ÐµÑ‚Ñ€Ð¸Ðº\n",
    "        metrics = {\"mae\": final_mae}\n",
    "        mlflow.log_metrics(metrics)\n",
    "        print(f\"Metrics logged: {metrics}\")\n",
    "        \n",
    "        # --- Ð›Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ ÑÐ°Ð¼Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ ---\n",
    "        mlflow.pytorch.log_model(\n",
    "            model,\n",
    "            artifact_path=\"lstm-model\"\n",
    "        )\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "    # # --- Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¿Ñ€Ð¾ÑÑ‚ÑƒÑŽ LSTM Ð¼Ð¾Ð´ÐµÐ»ÑŒ ---\n",
    "    # model = Sequential()\n",
    "    # model.add(LSTM(16, input_shape=(n_timesteps, n_features), return_sequences=True)) # return_sequences=True, ÐµÑÐ»Ð¸ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ ÑÐ»Ð¾Ð¹ Ñ‚Ð¾Ð¶Ðµ LSTM\n",
    "    # model.add(Dropout(0.2))\n",
    "    # model.add(LSTM(8))\n",
    "    # model.add(Dropout(0.2))\n",
    "    # model.add(Dense(1)) # ÐžÐ´Ð¸Ð½ Ð²Ñ‹Ñ…Ð¾Ð´, Ñ‚Ð°Ðº ÐºÐ°Ðº Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¾Ð´Ð½Ð¾ Ñ‡Ð¸ÑÐ»Ð¾ - RUL\n",
    "\n",
    "    # # ÐšÐ¾Ð¼Ð¿Ð¸Ð»Ð¸Ñ€ÑƒÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ\n",
    "    # model.compile(optimizer=model_params['optimizer'], loss=model_params['loss'], metrics=['mae'])\n",
    "\n",
    "    # summary_list = []\n",
    "    # model.summary(print_fn=lambda x: summary_list.append(x))\n",
    "    # model_summary_string = \"\\n\".join(summary_list)\n",
    "\n",
    "    # mlflow.log_text(model_summary_string, 'model_summary.txt')\n",
    "\n",
    "    # model.summary()\n",
    "\n",
    "    # # --- ÐžÐ±ÑƒÑ‡Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ ---\n",
    "    # history = model.fit(X_train, y_train, \n",
    "    #                     epochs=model_params['epochs'], \n",
    "    #                     batch_size=model_params['batch_size'], \n",
    "    #                     validation_split=model_params['validation_split'], # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ñ‡Ð°ÑÑ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸ Ð½Ð° Ð»ÐµÑ‚Ñƒ\n",
    "    #                     callbacks=[mlflow.keras.MLflowCallback()],\n",
    "    #                     verbose=1)\n",
    "\n",
    "    # # --- ÐžÑ†ÐµÐ½Ð¸Ð²Ð°ÐµÐ¼ Ð½Ð° Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… ---\n",
    "    # loss, mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "    # print(f'\\nTest MAE: {mae:.2f}')\n",
    "\n",
    "    # metrics = {\n",
    "    #     \"mae\": mae\n",
    "    # }\n",
    "    # mlflow.log_metrics(metrics)\n",
    "    # print(f\"Metrics logged: {metrics}\")\n",
    "\n",
    "    # # --- Ð›Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ ÑÐ°Ð¼Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ ---\n",
    "    # mlflow.keras.log_model(\n",
    "    #     model,\n",
    "    #     artifact_path=\"lstm-model\", # ÐÐ°Ð·Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð°Ð¿ÐºÐ¸ Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒÑŽ Ð² MLflow\n",
    "    # )\n",
    "\n",
    "print(\"Model logged as an artifact.\")\n",
    "\n",
    "print(\"MLflow run finished successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
