{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71c499bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import git\n",
    "import mlflow\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4eda345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. –ú–µ—Ç—Ä–∏–∫–∏ –∏ Scoring Functions ---\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    –í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –º–µ—Ç—Ä–∏–∫ –¥–ª—è RUL.\n",
    "    y_true, y_pred: torch.Tensor –∏–ª–∏ numpy array\n",
    "    \"\"\"\n",
    "    if isinstance(y_true, torch.Tensor):\n",
    "        y_true = y_true.detach().cpu().numpy()\n",
    "    if isinstance(y_pred, torch.Tensor):\n",
    "        y_pred = y_pred.detach().cpu().numpy()\n",
    "        \n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "    \n",
    "    # –†–∞–∑–Ω–∏—Ü–∞\n",
    "    d = y_pred - y_true\n",
    "    \n",
    "    # 1. MAE\n",
    "    mae = np.mean(np.abs(d))\n",
    "    \n",
    "    # 2. RMSE\n",
    "    rmse = np.sqrt(np.mean(d**2))\n",
    "    \n",
    "    # 3. MAPE\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1.0))) * 100\n",
    "\n",
    "    # 4. PHM08 Score (NASA Scoring Function) [web:PHM08_Challenge]\n",
    "    # –§—É–Ω–∫—Ü–∏—è –∞—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞: —Ä–∞–Ω–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (d < 0) —à—Ç—Ä–∞—Ñ—É—é—Ç—Å—è –º–µ–Ω—å—à–µ, —á–µ–º –ø–æ–∑–¥–Ω–∏–µ (d > 0)\n",
    "    # –§–æ—Ä–º—É–ª–∞: sum(exp(-d/13) - 1 –µ—Å–ª–∏ d < 0, –∏–Ω–∞—á–µ exp(d/10) - 1)\n",
    "    # *–í–Ω–∏–º–∞–Ω–∏–µ: –≤ —É—Å–ª–æ–≤–∏–∏ –±—ã–ª–æ —É–∫–∞–∑–∞–Ω–æ score = sum(...) / n. –û–±—ã—á–Ω–æ –≤ PHM08 –∏—Å–ø–æ–ª—å–∑—É—é—Ç –ø—Ä–æ—Å—Ç–æ sum,\n",
    "    # –Ω–æ –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ—Å—Ç–∏ –º–µ—Ç—Ä–∏–∫ –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ä–µ–¥–Ω–µ–µ (mean) –∏–ª–∏ —Å–ª–µ–¥–æ–≤–∞—Ç—å —É—Å–ª–æ–≤–∏—é –∑–∞–¥–∞—á–∏.\n",
    "    # –ó–¥–µ—Å—å —Ä–µ–∞–ª–∏–∑—É–µ–º —Å–æ–≥–ª–∞—Å–Ω–æ –≤–∞—à–µ–º—É –¢–ó: –¥–µ–ª–∏–º –Ω–∞ n.\n",
    "    n = len(d)\n",
    "    scores = np.where(d < 0, np.exp(-d/13) - 1, np.exp(d/10) - 1)\n",
    "    phm08_score = np.sum(scores) / n\n",
    "    \n",
    "    return {\"mae\": mae, \"rmse\": rmse, \"mape\": mape, \"phm08_score\": phm08_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5be19ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å (Transfer Learning Ready) ---\n",
    "\n",
    "class AdaptiveLSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_1=64, hidden_dim_2=32, output_dim=1, dropout_prob=0.2):\n",
    "        super(AdaptiveLSTMModel, self).__init__()\n",
    "        \n",
    "        # === –ê–î–ê–ü–¢–ò–í–ù–´–ô –í–•–û–î–ù–û–ô –°–õ–û–ô (ADAPTER) ===\n",
    "        # –ü—Ä–æ–µ—Ü–∏—Ä—É–µ–º –≤—Ö–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–ª—é–±–æ–≥–æ –∫–æ–ª-–≤–∞) –≤ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–∫—Ä—ã—Ç–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ (hidden_dim_1).\n",
    "        # –ü—Ä–∏ Transfer Learning –º—ã –∑–∞–º–µ–Ω–∏–º —Ç–æ–ª—å–∫–æ —ç—Ç–æ—Ç —Å–ª–æ–π.\n",
    "        self.input_adapter = nn.Linear(input_dim, hidden_dim_1)\n",
    "        \n",
    "        # === BACKBONE (–Ø–î–†–û –ú–û–î–ï–õ–ò) ===\n",
    "        # LSTM —Å–ª–æ–∏ —Ç–µ–ø–µ—Ä—å –ø—Ä–∏–Ω–∏–º–∞—é—Ç hidden_dim_1, –∞ –Ω–µ input_dim.\n",
    "        # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤–µ—Å–∞–º LSTM –æ—Å—Ç–∞–≤–∞—Ç—å—Å—è –≤–∞–ª–∏–¥–Ω—ã–º–∏ –¥–∞–∂–µ –µ—Å–ª–∏ input_dim –∏–∑–º–µ–Ω–∏—Ç—Å—è.\n",
    "        self.backbone_lstm1 = nn.LSTM(hidden_dim_1, hidden_dim_1, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.backbone_lstm2 = nn.LSTM(hidden_dim_1, hidden_dim_2, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        # –§–∏–Ω–∞–ª—å–Ω–∞—è \"–≥–æ–ª–æ–≤–∞\"\n",
    "        self.fc = nn.Linear(hidden_dim_2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_dim)\n",
    "        \n",
    "        # 1. –ê–¥–∞–ø—Ç–∞—Ü–∏—è –≤—Ö–æ–¥–∞\n",
    "        x_embedded = self.input_adapter(x) # -> (batch_size, seq_len, hidden_dim_1)\n",
    "        \n",
    "        # 2. –ü—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ Backbone\n",
    "        lstm1_out, _ = self.backbone_lstm1(x_embedded)\n",
    "        out = self.dropout1(lstm1_out)\n",
    "        \n",
    "        lstm2_out, _ = self.backbone_lstm2(out)\n",
    "        \n",
    "        # –ë–µ—Ä–µ–º –≤—ã—Ö–æ–¥ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —à–∞–≥–∞\n",
    "        last_hidden_state = lstm2_out[:, -1, :] \n",
    "        out = self.dropout2(last_hidden_state)\n",
    "        \n",
    "        # 3. –§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑\n",
    "        final_output = self.fc(out)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c430d1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Git Commit Hash: 068edc6a317958c4f3595aafd2a7c2df5c989afc\n"
     ]
    }
   ],
   "source": [
    "# –£–∫–∞–∑—ã–≤–∞–µ–º MLflow, –∫—É–¥–∞ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å –¥–∞–Ω–Ω—ã–µ\n",
    "mlflow.set_tracking_uri(\"http://213.21.252.250:5000\")\n",
    "\n",
    "# –ó–∞–¥–∞–µ–º –∏–º—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞\n",
    "mlflow.set_experiment(\"LSTM_TransferLearning_Ready\")\n",
    "\n",
    "# --- –ü–æ–ª—É—á–∞–µ–º —Ö–µ—à –∫–æ–º–º–∏—Ç–∞ Git ---\n",
    "try:\n",
    "    repo = git.Repo(search_parent_directories=True)\n",
    "    git_commit_hash = repo.head.object.hexsha\n",
    "except Exception as e:\n",
    "    git_commit_hash = \"N/A\" # –ù–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ —Å–∫—Ä–∏–ø—Ç –∑–∞–ø—É—â–µ–Ω –Ω–µ –∏–∑ Git-—Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è\n",
    "    print(f\"Warning: Could not get git commit hash. {e}\")\n",
    "\n",
    "print(f\"Current Git Commit Hash: {git_commit_hash}\")\n",
    "\n",
    "# --- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å ---\n",
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ —Å–∫—Ä–∏–ø—Ç–∞ –Ω–∞—Ä–µ–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö (sample_creator)\n",
    "data_params = {\n",
    "    \"window_size\": 50,\n",
    "    \"step\": 1,\n",
    "    \"sampling_rate\": 10\n",
    "}\n",
    "\n",
    "# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏\n",
    "model_params = {\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 128,\n",
    "    \"validation_split\": 0.2,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss\": \"mean_squared_error\",\n",
    "    \"lr\": 0.002,\n",
    "    \"hidden_dim_1\": 32,\n",
    "    \"hidden_dim_2\": 16,\n",
    "    \"dropout\": 0.2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c208c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/processed/Unit16_win50_str1_smp10.npz', '../data/processed/Unit5_win50_str1_smp10.npz', '../data/processed/Unit18_win50_str1_smp10.npz', '../data/processed/Unit20_win50_str1_smp10.npz', '../data/processed/Unit2_win50_str1_smp10.npz', '../data/processed/Unit10_win50_str1_smp10.npz']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(train_files)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m X_train, y_train = \u001b[43mload_and_merge_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m X_test, y_test = load_and_merge_data(test_files)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º—É –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ X_train\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mload_and_merge_data\u001b[39m\u001b[34m(npz_units)\u001b[39m\n\u001b[32m      6\u001b[39m   sample_array_lst.append(loaded[\u001b[33m'\u001b[39m\u001b[33msample\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      7\u001b[39m   label_array_lst.append(loaded[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m sample_array = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_array_lst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m label_array = np.concatenate(label_array_lst)\n\u001b[32m     10\u001b[39m sample_array = sample_array.transpose(\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/turbo-predict/.venv/lib/python3.12/site-packages/numpy/lib/shape_base.py:715\u001b[39m, in \u001b[36mdstack\u001b[39m\u001b[34m(tup)\u001b[39m\n\u001b[32m    713\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m    714\u001b[39m     arrs = [arrs]\n\u001b[32m--> \u001b[39m\u001b[32m715\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def load_and_merge_data(npz_units):\n",
    "      sample_array_lst = []\n",
    "      label_array_lst = []\n",
    "      for npz_unit in npz_units:\n",
    "        loaded = np.load(npz_unit)\n",
    "        sample_array_lst.append(loaded['sample'])\n",
    "        label_array_lst.append(loaded['label'])\n",
    "      sample_array = np.dstack(sample_array_lst)\n",
    "      label_array = np.concatenate(label_array_lst)\n",
    "      sample_array = sample_array.transpose(2, 0, 1)\n",
    "      return sample_array, label_array\n",
    "\n",
    "processed_dir = '../data/processed/'\n",
    "\n",
    "# –°–æ–±–∏—Ä–∞–µ–º –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º –¥–ª—è train –∏ test\n",
    "train_files = [os.path.join(processed_dir, f) for f in os.listdir(processed_dir) if f.startswith(('Unit2_', 'Unit5_', 'Unit10_', 'Unit16_', 'Unit18_', 'Unit20_'))]\n",
    "test_files = [os.path.join(processed_dir, f) for f in os.listdir(processed_dir) if f.startswith(('Unit11_', 'Unit14_', 'Unit15_'))]\n",
    "print(train_files)\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "X_train, y_train = load_and_merge_data(train_files)\n",
    "X_test, y_test = load_and_merge_data(test_files)\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º—É –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ X_train\n",
    "n_timesteps, n_features = X_train.shape[1], X_train.shape[2]\n",
    "\n",
    "print('–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏ (X):', X_train.shape)\n",
    "print('–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏ (y):', y_train.shape)\n",
    "print('–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ (X):', X_test.shape)\n",
    "print('–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ (y):', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ab700d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MLflow run...\n",
      "Parameters logged.\n",
      "Epoch 1/7 | Train Loss: 666.80 | Val MAE: 19.15 | PHM08: 8.05\n",
      "Epoch 2/7 | Train Loss: 518.58 | Val MAE: 19.15 | PHM08: 8.04\n",
      "Epoch 3/7 | Train Loss: 517.20 | Val MAE: 19.15 | PHM08: 8.05\n",
      "Epoch 4/7 | Train Loss: 414.75 | Val MAE: 7.32 | PHM08: 1.49\n",
      "Epoch 5/7 | Train Loss: 80.39 | Val MAE: 5.37 | PHM08: 0.84\n",
      "Epoch 6/7 | Train Loss: 72.07 | Val MAE: 4.74 | PHM08: 0.69\n",
      "Epoch 7/7 | Train Loss: 68.47 | Val MAE: 4.63 | PHM08: 0.62\n",
      "\n",
      "Evaluating on Test Set...\n",
      "Test Metrics: {'mae': 4.0629144, 'rmse': 5.3320966, 'mape': 36561420800.0, 'phm08_score': 0.5586638414639016}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 22:13:56 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\u001b[31m2025/12/20 22:14:00 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Complete. Artifacts logged.\n",
      "üèÉ View run thoughtful-wasp-743 at: http://213.21.252.250:5000/#/experiments/2/runs/3d9fc09d7c7d436ca54132cbd05102c6\n",
      "üß™ View experiment at: http://213.21.252.250:5000/#/experiments/2\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    print(\"Starting MLflow run...\")\n",
    "\n",
    "    # --- –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ---\n",
    "    mlflow.log_params(data_params)\n",
    "    mlflow.log_params(model_params)\n",
    "    mlflow.set_tag(\"git_commit\", git_commit_hash)\n",
    "    print(\"Parameters logged.\")\n",
    "\n",
    "    \n",
    "    # --- –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª—å LSTM –Ω–∞ PyTorch ---\n",
    "    class LSTMModel(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim_1, hidden_dim_2, output_dim=1, dropout_prob=0.2):\n",
    "            super(LSTMModel, self).__init__()\n",
    "            # –ü–µ—Ä–≤—ã–π LSTM —Å–ª–æ–π\n",
    "            self.lstm1 = nn.LSTM(input_dim, hidden_dim_1, batch_first=True)\n",
    "            # batch_first=True –æ—á–µ–Ω—å –≤–∞–∂–µ–Ω, —á—Ç–æ–±—ã –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–º–µ–ª–∏ —Ñ–æ—Ä–º–∞—Ç (batch, seq, feature), –∫–∞–∫ –≤ Keras\n",
    "            \n",
    "            self.dropout1 = nn.Dropout(dropout_prob)\n",
    "            \n",
    "            # –í—Ç–æ—Ä–æ–π LSTM —Å–ª–æ–π\n",
    "            # –û–Ω –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ—è (hidden_dim_1)\n",
    "            self.lstm2 = nn.LSTM(hidden_dim_1, hidden_dim_2, batch_first=True)\n",
    "            \n",
    "            self.dropout2 = nn.Dropout(dropout_prob)\n",
    "            \n",
    "            # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞\n",
    "            self.fc = nn.Linear(hidden_dim_2, output_dim)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # –ü–µ—Ä–≤—ã–π LSTM —Å–ª–æ–π\n",
    "            # LSTM –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç output –∏ –∫–æ—Ä—Ç–µ–∂ (hidden_state, cell_state)\n",
    "            # –ù–∞–º –Ω—É–∂–µ–Ω output –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ª–æ—è\n",
    "            lstm1_out, _ = self.lstm1(x)\n",
    "            \n",
    "            # Dropout\n",
    "            out = self.dropout1(lstm1_out)\n",
    "            \n",
    "            # –í—Ç–æ—Ä–æ–π LSTM —Å–ª–æ–π\n",
    "            # –ù–∞–º –Ω—É–∂–µ–Ω —Ç–æ–ª—å–∫–æ –≤—ã—Ö–æ–¥ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —à–∞–≥–∞\n",
    "            lstm2_out, _ = self.lstm2(out)\n",
    "            last_hidden_state = lstm2_out[:, -1, :] # –ë–µ—Ä–µ–º –≤—ã—Ö–æ–¥ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "            \n",
    "            # Dropout\n",
    "            out = self.dropout2(last_hidden_state)\n",
    "            \n",
    "            # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π\n",
    "            final_output = self.fc(out)\n",
    "            return final_output\n",
    "\n",
    "    # --- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è PyTorch ---\n",
    "    # 1. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º numpy –º–∞—Å—Å–∏–≤—ã –≤ torch —Ç–µ–Ω–∑–æ—Ä—ã\n",
    "    X_train_tensor = torch.from_numpy(X_train).float()\n",
    "    y_train_tensor = torch.from_numpy(y_train).float().view(-1, 1) # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ —Ñ–æ—Ä–º–∞ (batch_size, 1)\n",
    "    X_test_tensor = torch.from_numpy(X_test).float()\n",
    "    y_test_tensor = torch.from_numpy(y_test).float().view(-1, 1)\n",
    "\n",
    "    # 2. –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    # 3. –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫–∏ –≤—Ä—É—á–Ω—É—é\n",
    "    val_split = model_params['validation_split']\n",
    "    dataset_size = len(train_dataset)\n",
    "    val_size = int(val_split * dataset_size)\n",
    "    train_size = dataset_size - val_size\n",
    "    train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "    # 4. –°–æ–∑–¥–∞–µ–º –∑–∞–≥—Ä—É–∑—á–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö (DataLoader), –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –ø–æ–¥–∞–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –±–∞—Ç—á–∞–º–∏\n",
    "    train_loader = DataLoader(dataset=train_subset, batch_size=model_params['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(dataset=val_subset, batch_size=model_params['batch_size'])\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=model_params['batch_size'])\n",
    "\n",
    "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏\n",
    "    device = torch.device(\"cpu\") # –¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ: —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ CPU\n",
    "    \n",
    "    model = AdaptiveLSTMModel(\n",
    "        input_dim=n_features,\n",
    "        hidden_dim_1=model_params[\"hidden_dim_1\"],\n",
    "        hidden_dim_2=model_params[\"hidden_dim_2\"],\n",
    "        dropout_prob=model_params[\"dropout\"]\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=model_params[\"lr\"])\n",
    "\n",
    "    # –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è\n",
    "    for epoch in range(model_params[\"epochs\"]):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        mlflow.log_metric(\"train_mse_loss\", avg_train_loss, step=epoch)\n",
    "\n",
    "        # –í–∞–ª–∏–¥–∞—Ü–∏—è\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_targets = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                val_preds.append(outputs.numpy())\n",
    "                val_targets.append(labels.numpy())\n",
    "        \n",
    "        val_preds = np.concatenate(val_preds)\n",
    "        val_targets = np.concatenate(val_targets)\n",
    "        \n",
    "        # –†–∞—Å—á–µ—Ç –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫\n",
    "        val_metrics = calculate_metrics(val_targets, val_preds)\n",
    "        \n",
    "        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "        for name, value in val_metrics.items():\n",
    "            mlflow.log_metric(f\"val_{name}\", value, step=epoch)\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}/{model_params['epochs']} | Train Loss: {avg_train_loss:.2f} | Val MAE: {val_metrics['mae']:.2f} | PHM08: {val_metrics['phm08_score']:.2f}\")\n",
    "\n",
    "    # --- 5. –§–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ---\n",
    "    print(\"\\nEvaluating on Test Set...\")\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    test_targets = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            test_preds.append(outputs.numpy())\n",
    "            test_targets.append(labels.numpy())\n",
    "    \n",
    "    test_preds = np.concatenate(test_preds)\n",
    "    test_targets = np.concatenate(test_targets)\n",
    "    \n",
    "    test_metrics = calculate_metrics(test_targets, test_preds)\n",
    "    print(f\"Test Metrics: {test_metrics}\")\n",
    "    \n",
    "    # –õ–æ–≥–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º test_\n",
    "    for name, value in test_metrics.items():\n",
    "        mlflow.log_metric(f\"test_{name}\", value)\n",
    "\n",
    "    # 1. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ü–û–õ–ù–û–ô –º–æ–¥–µ–ª–∏\n",
    "    mlflow.pytorch.log_model(model, \"full_model\")\n",
    "    \n",
    "    # 2. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ BACKBONE (State Dict –±–µ–∑ –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è) –¥–ª—è Transfer Learning\n",
    "    # –ò—Å–∫–ª—é—á–∞–µ–º –≤–µ—Å–∞ input_adapter, —á—Ç–æ–±—ã –∫–ª–∏–µ–Ω—Ç –º–æ–≥ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ–∏\n",
    "    backbone_state_dict = {k: v for k, v in model.state_dict().items() if \"input_adapter\" not in k}\n",
    "    torch.save(backbone_state_dict, \"backbone.pth\")\n",
    "    mlflow.log_artifact(\"backbone.pth\", artifact_path=\"transfer_learning_artifacts\")\n",
    "    \n",
    "    print(\"Run Complete. Artifacts logged.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
