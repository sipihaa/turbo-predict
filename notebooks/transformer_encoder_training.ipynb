{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c499bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import git\n",
    "import mlflow\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4eda345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. –ú–µ—Ç—Ä–∏–∫–∏ –∏ Scoring Functions ---\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    –í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –º–µ—Ç—Ä–∏–∫ –¥–ª—è RUL.\n",
    "    y_true, y_pred: torch.Tensor –∏–ª–∏ numpy array\n",
    "    \"\"\"\n",
    "    if isinstance(y_true, torch.Tensor):\n",
    "        y_true = y_true.detach().cpu().numpy()\n",
    "    if isinstance(y_pred, torch.Tensor):\n",
    "        y_pred = y_pred.detach().cpu().numpy()\n",
    "        \n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "    \n",
    "    # –†–∞–∑–Ω–∏—Ü–∞\n",
    "    d = y_pred - y_true\n",
    "    \n",
    "    # 1. MAE\n",
    "    mae = np.mean(np.abs(d))\n",
    "    \n",
    "    # 2. RMSE\n",
    "    rmse = np.sqrt(np.mean(d**2))\n",
    "    \n",
    "    # 3. MAPE\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1.0))) * 100\n",
    "\n",
    "    # 4. PHM08 Score (NASA Scoring Function) [web:PHM08_Challenge]\n",
    "    # –§—É–Ω–∫—Ü–∏—è –∞—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞: —Ä–∞–Ω–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (d < 0) —à—Ç—Ä–∞—Ñ—É—é—Ç—Å—è –º–µ–Ω—å—à–µ, —á–µ–º –ø–æ–∑–¥–Ω–∏–µ (d > 0)\n",
    "    # –§–æ—Ä–º—É–ª–∞: sum(exp(-d/13) - 1 –µ—Å–ª–∏ d < 0, –∏–Ω–∞—á–µ exp(d/10) - 1)\n",
    "    # *–í–Ω–∏–º–∞–Ω–∏–µ: –≤ —É—Å–ª–æ–≤–∏–∏ –±—ã–ª–æ —É–∫–∞–∑–∞–Ω–æ score = sum(...) / n. –û–±—ã—á–Ω–æ –≤ PHM08 –∏—Å–ø–æ–ª—å–∑—É—é—Ç –ø—Ä–æ—Å—Ç–æ sum,\n",
    "    # –Ω–æ –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ—Å—Ç–∏ –º–µ—Ç—Ä–∏–∫ –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ä–µ–¥–Ω–µ–µ (mean) –∏–ª–∏ —Å–ª–µ–¥–æ–≤–∞—Ç—å —É—Å–ª–æ–≤–∏—é –∑–∞–¥–∞—á–∏.\n",
    "    # –ó–¥–µ—Å—å —Ä–µ–∞–ª–∏–∑—É–µ–º —Å–æ–≥–ª–∞—Å–Ω–æ –≤–∞—à–µ–º—É –¢–ó: –¥–µ–ª–∏–º –Ω–∞ n.\n",
    "    n = len(d)\n",
    "    scores = np.where(d < 0, np.exp(-d/13) - 1, np.exp(d/10) - 1)\n",
    "    phm08_score = np.sum(scores) / n\n",
    "    \n",
    "    return {\"mae\": mae, \"rmse\": rmse, \"mape\": mape, \"phm08_score\": phm08_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5be19ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å (Transfer Learning Ready) ---\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    –î–æ–±–∞–≤–ª—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–∑–∏—Ü–∏–∏ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\n",
    "    –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∏–∑ PyTorch —Ç—É—Ç–æ—Ä–∏–∞–ª–æ–≤.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É –ø–æ–∑–∏—Ü–∏–π\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –±–∞—Ç—á–∞: (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (Batch, Seq_Len, Embed_Dim)\n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é –∫ –≤—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º (—Å—Ä–µ–∑–∞–µ–º pe –ø–æ–¥ –¥–ª–∏–Ω—É —Ç–µ–∫—É—â–µ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class AdaptiveTransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim=64, num_heads=4, ff_dim=128, num_layers=2, output_dim=1, dropout=0.1):\n",
    "        super(AdaptiveTransformerModel, self).__init__()\n",
    "        \n",
    "        # === 1. –ê–î–ê–ü–¢–ò–í–ù–´–ô –í–•–û–î–ù–û–ô –°–õ–û–ô (ADAPTER) ===\n",
    "        # –ü—Ä–æ–µ—Ü–∏—Ä—É–µ–º N —Ñ–∏—á–µ–π –≤ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ (embed_dim).\n",
    "        # –≠—Ç–æ —Å–º–µ–Ω–Ω–∞—è —á–∞—Å—Ç—å –¥–ª—è Transfer Learning.\n",
    "        self.input_adapter = nn.Linear(input_dim, embed_dim)\n",
    "        \n",
    "        # === 2. BACKBONE (–Ø–î–†–û: TRANSFORMER) ===\n",
    "        \n",
    "        # –∞) –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–∏\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, dropout=dropout)\n",
    "        \n",
    "        # –±) –ë–ª–æ–∫ –≠–Ω–∫–æ–¥–µ—Ä–∞\n",
    "        # batch_first=True –æ–∑–Ω–∞—á–∞–µ—Ç –≤—Ö–æ–¥ (Batch, Seq, Feature)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=ff_dim, \n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # === 3. –ì–û–õ–û–í–ê (HEAD) ===\n",
    "        # –ü—Ä–∏–Ω–∏–º–∞–µ—Ç —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä (Global Average Pooling)\n",
    "        self.fc = nn.Linear(embed_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_dim)\n",
    "        \n",
    "        # 1. –ê–¥–∞–ø—Ç–∞—Ü–∏—è + –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "        # –í —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö –ø—Ä–∏–Ω—è—Ç–æ —É–º–Ω–æ–∂–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–∞ sqrt(d_model) –ø–µ—Ä–µ–¥ –ø–æ–∑–∏—Ü–∏–µ–π\n",
    "        x = self.input_adapter(x) * math.sqrt(self.input_adapter.out_features)\n",
    "        \n",
    "        # 2. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–æ—Ä—è–¥–∫–µ –≤—Ä–µ–º–µ–Ω–∏\n",
    "        x = self.pos_encoder(x)\n",
    "        # Shape: (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        # 3. –ü—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ Transformer Encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "        # Shape: (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        # 4. Global Average Pooling 1D\n",
    "        # –£—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –æ—Å–∏ (dim=1)\n",
    "        # –ë—ã–ª–æ: (Batch, Seq, Emb) -> –°—Ç–∞–ª–æ: (Batch, Emb)\n",
    "        x = x.mean(dim=1) \n",
    "        \n",
    "        # 5. –§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑\n",
    "        final_output = self.fc(x)\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469043a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_for_customer_data(\n",
    "    backbone_path,\n",
    "    new_input_dim,\n",
    "    embed_dim=64,\n",
    "    num_heads=4,\n",
    "    ff_dim=128,\n",
    "    num_layers=2,\n",
    "    dropout=0.1,\n",
    "    output_dim=1,\n",
    "    freeze_backbone=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    –°–æ–∑–¥–∞–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∑–∞–∫–∞–∑—á–∏–∫–∞, –∑–∞–≥—Ä—É–∂–∞—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–π Transformer-backbone.\n",
    "    \"\"\"\n",
    "    # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å —Å –ù–û–í–û–ô —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é –≤—Ö–æ–¥–∞\n",
    "    new_model = AdaptiveTransformerModel(\n",
    "        input_dim=new_input_dim,  # –ù–∞–ø—Ä–∏–º–µ—Ä, 15 —Å–µ–Ω—Å–æ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 20\n",
    "        embed_dim=embed_dim,\n",
    "        num_heads=num_heads,\n",
    "        ff_dim=ff_dim,\n",
    "        num_layers=num_layers,\n",
    "        output_dim=output_dim,\n",
    "        dropout=dropout,\n",
    "    )\n",
    "\n",
    "    # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ Backbone\n",
    "    # strict=False –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–ª—é—á–µ–π (—Ç.–∫. input_adapter —É –Ω–∞—Å –Ω–æ–≤—ã–π)\n",
    "    pretrained_dict = torch.load(backbone_path, map_location=\"cpu\")\n",
    "    model_dict = new_model.state_dict()\n",
    "\n",
    "    # –§–∏–ª—å—Ç—Ä—É–µ–º, —á—Ç–æ–±—ã –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–æ–ª—å–∫–æ backbone\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and \"input_adapter\" not in k}\n",
    "\n",
    "    # –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞\n",
    "    model_dict.update(pretrained_dict)\n",
    "    new_model.load_state_dict(model_dict, strict=False)\n",
    "\n",
    "    # 3. –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º Backbone (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –µ—Å–ª–∏ —Ö–æ—Ç–∏–º —É—á–∏—Ç—å —Ç–æ–ª—å–∫–æ –∞–¥–∞–ø—Ç–µ—Ä)\n",
    "    if freeze_backbone:\n",
    "        for name, param in new_model.named_parameters():\n",
    "            if \"input_adapter\" not in name:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    print(\n",
    "        f\"Model adapted for input_dim={new_input_dim}. Backbone weights loaded\"\n",
    "        f\"{' and frozen' if freeze_backbone else ''}.\"\n",
    "    )\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c430d1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Git Commit Hash: 10e7174a76b97d41dbe6a5b23052a7fc80aa8ee5\n"
     ]
    }
   ],
   "source": [
    "# –£–∫–∞–∑—ã–≤–∞–µ–º MLflow, –∫—É–¥–∞ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å –¥–∞–Ω–Ω—ã–µ\n",
    "mlflow.set_tracking_uri(\"http://213.21.252.250:5000\")\n",
    "\n",
    "# –ó–∞–¥–∞–µ–º –∏–º—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞\n",
    "mlflow.set_experiment(\"Transformer_TransferLearning\")\n",
    "\n",
    "# --- –ü–æ–ª—É—á–∞–µ–º —Ö–µ—à –∫–æ–º–º–∏—Ç–∞ Git ---\n",
    "try:\n",
    "    repo = git.Repo(search_parent_directories=True)\n",
    "    git_commit_hash = repo.head.object.hexsha\n",
    "except Exception as e:\n",
    "    git_commit_hash = \"N/A\"  # –ù–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ —Å–∫—Ä–∏–ø—Ç –∑–∞–ø—É—â–µ–Ω –Ω–µ –∏–∑ Git-—Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è\n",
    "    print(f\"Warning: Could not get git commit hash. {e}\")\n",
    "\n",
    "print(f\"Current Git Commit Hash: {git_commit_hash}\")\n",
    "\n",
    "# --- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å ---\n",
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ —Å–∫—Ä–∏–ø—Ç–∞ –Ω–∞—Ä–µ–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö (sample_creator)\n",
    "data_params = {\n",
    "    \"window_size\": 100,\n",
    "    \"step\": 1,\n",
    "    \"sampling_rate\": 10,\n",
    "}\n",
    "\n",
    "# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ (Transformer)\n",
    "model_params = {\n",
    "    \"epochs\": 7,\n",
    "    \"batch_size\": 128,\n",
    "    \"validation_split\": 0.2,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss\": \"mean_squared_error\",\n",
    "    \"lr\": 0.002,\n",
    "    \"embed_dim\": 64,\n",
    "    \"num_heads\": 4,\n",
    "    \"ff_dim\": 128,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c208c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/processed/Unit5_win100_str1_smp10.npz', '../data/processed/Unit16_win100_str1_smp10.npz', '../data/processed/Unit2_win100_str1_smp10.npz', '../data/processed/Unit20_win100_str1_smp10.npz', '../data/processed/Unit18_win100_str1_smp10.npz', '../data/processed/Unit10_win100_str1_smp10.npz']\n",
      "–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏ (X): (525751, 100, 20)\n",
      "–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏ (y): (525751,)\n",
      "–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ (X): (125077, 100, 20)\n",
      "–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ (y): (125077,)\n"
     ]
    }
   ],
   "source": [
    "def load_and_merge_data(npz_units):\n",
    "      sample_array_lst = []\n",
    "      label_array_lst = []\n",
    "      for npz_unit in npz_units:\n",
    "        loaded = np.load(npz_unit)\n",
    "        sample_array_lst.append(loaded['sample'])\n",
    "        label_array_lst.append(loaded['label'])\n",
    "      sample_array = np.dstack(sample_array_lst)\n",
    "      label_array = np.concatenate(label_array_lst)\n",
    "      sample_array = sample_array.transpose(2, 0, 1)\n",
    "      return sample_array, label_array\n",
    "\n",
    "processed_dir = '../data/processed/'\n",
    "\n",
    "# –°–æ–±–∏—Ä–∞–µ–º –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º –¥–ª—è train –∏ test\n",
    "train_files = [os.path.join(processed_dir, f) for f in os.listdir(processed_dir) if f.startswith(('Unit2_', 'Unit5_', 'Unit10_', 'Unit16_', 'Unit18_', 'Unit20_'))]\n",
    "test_files = [os.path.join(processed_dir, f) for f in os.listdir(processed_dir) if f.startswith(('Unit11_', 'Unit14_', 'Unit15_'))]\n",
    "print(train_files)\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "X_train, y_train = load_and_merge_data(train_files)\n",
    "X_test, y_test = load_and_merge_data(test_files)\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º—É –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ X_train\n",
    "n_timesteps, n_features = X_train.shape[1], X_train.shape[2]\n",
    "\n",
    "print('–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏ (X):', X_train.shape)\n",
    "print('–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏ (y):', y_train.shape)\n",
    "print('–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ (X):', X_test.shape)\n",
    "print('–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ (y):', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ab700d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MLflow run...\n",
      "Parameters logged.\n",
      "Epoch 1/7 | Train Loss: 536.98 | Val MAE: 19.17 | PHM08: 8.01\n",
      "Epoch 2/7 | Train Loss: 504.73 | Val MAE: 19.18 | PHM08: 8.25\n",
      "Epoch 3/7 | Train Loss: 504.41 | Val MAE: 19.17 | PHM08: 8.04\n",
      "Epoch 4/7 | Train Loss: 504.67 | Val MAE: 19.16 | PHM08: 8.10\n",
      "Epoch 5/7 | Train Loss: 80.72 | Val MAE: 4.65 | PHM08: 0.68\n",
      "Epoch 6/7 | Train Loss: 52.74 | Val MAE: 4.62 | PHM08: 0.68\n",
      "Epoch 7/7 | Train Loss: 51.14 | Val MAE: 5.06 | PHM08: 0.81\n",
      "\n",
      "Evaluating on Test Set...\n",
      "Test Metrics: {'mae': 6.7564287, 'rmse': 8.305587, 'mape': 42.46154427528381, 'phm08_score': 1.2659148214699745}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/21 14:19:54 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\u001b[31m2025/12/21 14:19:57 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Complete. Artifacts logged.\n",
      "üèÉ View run grandiose-skunk-663 at: http://213.21.252.250:5000/#/experiments/3/runs/f3f721f3dfb142b681e1771ead3493b7\n",
      "üß™ View experiment at: http://213.21.252.250:5000/#/experiments/3\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    print(\"Starting MLflow run...\")\n",
    "\n",
    "    # --- –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ---\n",
    "    mlflow.log_params(data_params)\n",
    "    mlflow.log_params(model_params)\n",
    "    mlflow.set_tag(\"git_commit\", git_commit_hash)\n",
    "    print(\"Parameters logged.\")\n",
    "\n",
    "    # --- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è PyTorch ---\n",
    "    # 1. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º numpy –º–∞—Å—Å–∏–≤—ã –≤ torch —Ç–µ–Ω–∑–æ—Ä—ã\n",
    "    X_train_tensor = torch.from_numpy(X_train).float()\n",
    "    y_train_tensor = torch.from_numpy(y_train).float().view(-1, 1)  # (batch_size, 1)\n",
    "    X_test_tensor = torch.from_numpy(X_test).float()\n",
    "    y_test_tensor = torch.from_numpy(y_test).float().view(-1, 1)\n",
    "\n",
    "    # 2. –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    # 3. –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫–∏ –≤—Ä—É—á–Ω—É—é\n",
    "    val_split = model_params[\"validation_split\"]\n",
    "    dataset_size = len(train_dataset)\n",
    "    val_size = int(val_split * dataset_size)\n",
    "    train_size = dataset_size - val_size\n",
    "    train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "    # 4. –°–æ–∑–¥–∞–µ–º –∑–∞–≥—Ä—É–∑—á–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö (DataLoader)\n",
    "    train_loader = DataLoader(dataset=train_subset, batch_size=model_params[\"batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(dataset=val_subset, batch_size=model_params[\"batch_size\"])\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=model_params[\"batch_size\"])\n",
    "\n",
    "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏\n",
    "    device = torch.device(\"cpu\")  # –¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ: —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ CPU\n",
    "\n",
    "    model = AdaptiveTransformerModel(\n",
    "        input_dim=n_features,\n",
    "        embed_dim=model_params[\"embed_dim\"],\n",
    "        num_heads=model_params[\"num_heads\"],\n",
    "        ff_dim=model_params[\"ff_dim\"],\n",
    "        num_layers=model_params[\"num_layers\"],\n",
    "        output_dim=1,\n",
    "        dropout=model_params[\"dropout\"],\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=model_params[\"lr\"])\n",
    "\n",
    "    # –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è\n",
    "    for epoch in range(model_params[\"epochs\"]):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        avg_train_loss = float(np.mean(train_losses))\n",
    "        mlflow.log_metric(\"train_mse_loss\", avg_train_loss, step=epoch)\n",
    "\n",
    "        # –í–∞–ª–∏–¥–∞—Ü–∏—è\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_targets = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                val_preds.append(outputs.detach().cpu().numpy())\n",
    "                val_targets.append(labels.detach().cpu().numpy())\n",
    "\n",
    "        val_preds = np.concatenate(val_preds)\n",
    "        val_targets = np.concatenate(val_targets)\n",
    "\n",
    "        # –†–∞—Å—á–µ—Ç –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫\n",
    "        val_metrics = calculate_metrics(val_targets, val_preds)\n",
    "\n",
    "        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "        for name, value in val_metrics.items():\n",
    "            mlflow.log_metric(f\"val_{name}\", value, step=epoch)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{model_params['epochs']} | \"\n",
    "            f\"Train Loss: {avg_train_loss:.2f} | \"\n",
    "            f\"Val MAE: {val_metrics['mae']:.2f} | \"\n",
    "            f\"PHM08: {val_metrics['phm08_score']:.2f}\"\n",
    "        )\n",
    "\n",
    "    # --- –§–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ---\n",
    "    print(\"\\nEvaluating on Test Set...\")\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    test_targets = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            test_preds.append(outputs.detach().cpu().numpy())\n",
    "            test_targets.append(labels.detach().cpu().numpy())\n",
    "\n",
    "    test_preds = np.concatenate(test_preds)\n",
    "    test_targets = np.concatenate(test_targets)\n",
    "\n",
    "    test_metrics = calculate_metrics(test_targets, test_preds)\n",
    "    print(f\"Test Metrics: {test_metrics}\")\n",
    "\n",
    "    # –õ–æ–≥–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º test_\n",
    "    for name, value in test_metrics.items():\n",
    "        mlflow.log_metric(f\"test_{name}\", value)\n",
    "\n",
    "    # 1. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ü–û–õ–ù–û–ô –º–æ–¥–µ–ª–∏\n",
    "    mlflow.pytorch.log_model(model, \"full_model\")\n",
    "\n",
    "    # 2. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ BACKBONE (State Dict –±–µ–∑ –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è) –¥–ª—è Transfer Learning\n",
    "    # –ò—Å–∫–ª—é—á–∞–µ–º –≤–µ—Å–∞ input_adapter, —á—Ç–æ–±—ã –∫–ª–∏–µ–Ω—Ç –º–æ–≥ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ–∏\n",
    "    backbone_state_dict = {k: v for k, v in model.state_dict().items() if \"input_adapter\" not in k}\n",
    "    torch.save(backbone_state_dict, \"transformer_backbone.pth\")\n",
    "    mlflow.log_artifact(\"transformer_backbone.pth\", artifact_path=\"transfer_learning_artifacts\")\n",
    "\n",
    "    print(\"Run Complete. Artifacts logged.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
