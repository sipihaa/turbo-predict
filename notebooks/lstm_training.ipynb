{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "71c499bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import git\n",
    "import mlflow\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4eda345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. –ú–µ—Ç—Ä–∏–∫–∏ –∏ Scoring Functions ---\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    –í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –º–µ—Ç—Ä–∏–∫ –¥–ª—è RUL.\n",
    "    y_true, y_pred: torch.Tensor –∏–ª–∏ numpy array\n",
    "    \"\"\"\n",
    "    if isinstance(y_true, torch.Tensor):\n",
    "        y_true = y_true.detach().cpu().numpy()\n",
    "    if isinstance(y_pred, torch.Tensor):\n",
    "        y_pred = y_pred.detach().cpu().numpy()\n",
    "        \n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "    \n",
    "    # –†–∞–∑–Ω–∏—Ü–∞\n",
    "    d = y_pred - y_true\n",
    "    \n",
    "    # 1. MAE\n",
    "    mae = np.mean(np.abs(d))\n",
    "    \n",
    "    # 2. RMSE\n",
    "    rmse = np.sqrt(np.mean(d**2))\n",
    "    \n",
    "    # 3. MAPE\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1.0))) * 100\n",
    "\n",
    "    # 4. PHM08 Score (NASA Scoring Function) [web:PHM08_Challenge]\n",
    "    # –§—É–Ω–∫—Ü–∏—è –∞—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞: —Ä–∞–Ω–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (d < 0) —à—Ç—Ä–∞—Ñ—É—é—Ç—Å—è –º–µ–Ω—å—à–µ, —á–µ–º –ø–æ–∑–¥–Ω–∏–µ (d > 0)\n",
    "    # –§–æ—Ä–º—É–ª–∞: sum(exp(-d/13) - 1 –µ—Å–ª–∏ d < 0, –∏–Ω–∞—á–µ exp(d/10) - 1)\n",
    "    # *–í–Ω–∏–º–∞–Ω–∏–µ: –≤ —É—Å–ª–æ–≤–∏–∏ –±—ã–ª–æ —É–∫–∞–∑–∞–Ω–æ score = sum(...) / n. –û–±—ã—á–Ω–æ –≤ PHM08 –∏—Å–ø–æ–ª—å–∑—É—é—Ç –ø—Ä–æ—Å—Ç–æ sum,\n",
    "    # –Ω–æ –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ—Å—Ç–∏ –º–µ—Ç—Ä–∏–∫ –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ä–µ–¥–Ω–µ–µ (mean) –∏–ª–∏ —Å–ª–µ–¥–æ–≤–∞—Ç—å —É—Å–ª–æ–≤–∏—é –∑–∞–¥–∞—á–∏.\n",
    "    # –ó–¥–µ—Å—å —Ä–µ–∞–ª–∏–∑—É–µ–º —Å–æ–≥–ª–∞—Å–Ω–æ –≤–∞—à–µ–º—É –¢–ó: –¥–µ–ª–∏–º –Ω–∞ n.\n",
    "    n = len(d)\n",
    "    scores = np.where(d < 0, np.exp(-d/13) - 1, np.exp(d/10) - 1)\n",
    "    phm08_score = np.sum(scores) / n\n",
    "    \n",
    "    return {\"mae\": mae, \"rmse\": rmse, \"mape\": mape, \"phm08_score\": phm08_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a5be19ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å (Transfer Learning Ready) ---\n",
    "\n",
    "class AdaptiveLSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_1=64, hidden_dim_2=32, output_dim=1, dropout_prob=0.2):\n",
    "        super(AdaptiveLSTMModel, self).__init__()\n",
    "        \n",
    "        # === –ê–î–ê–ü–¢–ò–í–ù–´–ô –í–•–û–î–ù–û–ô –°–õ–û–ô (ADAPTER) ===\n",
    "        # –ü—Ä–æ–µ—Ü–∏—Ä—É–µ–º –≤—Ö–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–ª—é–±–æ–≥–æ –∫–æ–ª-–≤–∞) –≤ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–∫—Ä—ã—Ç–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ (hidden_dim_1).\n",
    "        # –ü—Ä–∏ Transfer Learning –º—ã –∑–∞–º–µ–Ω–∏–º —Ç–æ–ª—å–∫–æ —ç—Ç–æ—Ç —Å–ª–æ–π.\n",
    "        self.input_adapter = nn.Linear(input_dim, hidden_dim_1)\n",
    "        \n",
    "        # === BACKBONE (–Ø–î–†–û –ú–û–î–ï–õ–ò) ===\n",
    "        # LSTM —Å–ª–æ–∏ —Ç–µ–ø–µ—Ä—å –ø—Ä–∏–Ω–∏–º–∞—é—Ç hidden_dim_1, –∞ –Ω–µ input_dim.\n",
    "        # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤–µ—Å–∞–º LSTM –æ—Å—Ç–∞–≤–∞—Ç—å—Å—è –≤–∞–ª–∏–¥–Ω—ã–º–∏ –¥–∞–∂–µ –µ—Å–ª–∏ input_dim –∏–∑–º–µ–Ω–∏—Ç—Å—è.\n",
    "        self.backbone_lstm1 = nn.LSTM(hidden_dim_1, hidden_dim_1, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.backbone_lstm2 = nn.LSTM(hidden_dim_1, hidden_dim_2, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        # –§–∏–Ω–∞–ª—å–Ω–∞—è \"–≥–æ–ª–æ–≤–∞\"\n",
    "        self.fc = nn.Linear(hidden_dim_2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_dim)\n",
    "        \n",
    "        # 1. –ê–¥–∞–ø—Ç–∞—Ü–∏—è –≤—Ö–æ–¥–∞\n",
    "        x_embedded = self.input_adapter(x) # -> (batch_size, seq_len, hidden_dim_1)\n",
    "        \n",
    "        # 2. –ü—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ Backbone\n",
    "        lstm1_out, _ = self.backbone_lstm1(x_embedded)\n",
    "        out = self.dropout1(lstm1_out)\n",
    "        \n",
    "        lstm2_out, _ = self.backbone_lstm2(out)\n",
    "        \n",
    "        # –ë–µ—Ä–µ–º –≤—ã—Ö–æ–¥ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —à–∞–≥–∞\n",
    "        last_hidden_state = lstm2_out[:, -1, :] \n",
    "        out = self.dropout2(last_hidden_state)\n",
    "        \n",
    "        # 3. –§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑\n",
    "        final_output = self.fc(out)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "469043a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_for_customer_data(backbone_path, new_input_dim, hidden_dim_1=64, hidden_dim_2=32, dropout_prob=0.2):\n",
    "    \"\"\"\n",
    "    –°–æ–∑–¥–∞–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∑–∞–∫–∞–∑—á–∏–∫–∞, –∑–∞–≥—Ä—É–∂–∞—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–π backbone.\n",
    "    \"\"\"\n",
    "    # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å —Å –ù–û–í–û–ô —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é –≤—Ö–æ–¥–∞\n",
    "    new_model = AdaptiveLSTMModel(\n",
    "        input_dim=new_input_dim, # –ù–∞–ø—Ä–∏–º–µ—Ä, 15 —Å–µ–Ω—Å–æ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 20\n",
    "        hidden_dim_1=hidden_dim_1,\n",
    "        hidden_dim_2=hidden_dim_2,\n",
    "        dropout_prob=dropout_prob\n",
    "    )\n",
    "    \n",
    "    # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ Backbone\n",
    "    # strict=False –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–ª—é—á–µ–π (—Ç.–∫. input_adapter —É –Ω–∞—Å –Ω–æ–≤—ã–π)\n",
    "    pretrained_dict = torch.load(backbone_path)\n",
    "    model_dict = new_model.state_dict()\n",
    "    \n",
    "    # –§–∏–ª—å—Ç—Ä—É–µ–º, —á—Ç–æ–±—ã –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–æ–ª—å–∫–æ backbone\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and \"input_adapter\" not in k}\n",
    "    \n",
    "    # –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞\n",
    "    model_dict.update(pretrained_dict)\n",
    "    new_model.load_state_dict(model_dict)\n",
    "    \n",
    "    # 3. –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º Backbone (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –µ—Å–ª–∏ —Ö–æ—Ç–∏–º —É—á–∏—Ç—å —Ç–æ–ª—å–∫–æ –∞–¥–∞–ø—Ç–µ—Ä)\n",
    "    for name, param in new_model.named_parameters():\n",
    "        if \"input_adapter\" not in name:\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    print(f\"Model adapted for input_dim={new_input_dim}. Backbone weights loaded and frozen.\")\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c430d1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Git Commit Hash: 910fc0116571f29bf7c5df3b283d99b529fee69e\n"
     ]
    }
   ],
   "source": [
    "# –£–∫–∞–∑—ã–≤–∞–µ–º MLflow, –∫—É–¥–∞ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å –¥–∞–Ω–Ω—ã–µ\n",
    "mlflow.set_tracking_uri(\"http://213.21.252.250:5000\")\n",
    "\n",
    "# –ó–∞–¥–∞–µ–º –∏–º—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞\n",
    "mlflow.set_experiment(\"LSTM_TransferLearning_Ready\")\n",
    "\n",
    "# --- –ü–æ–ª—É—á–∞–µ–º —Ö–µ—à –∫–æ–º–º–∏—Ç–∞ Git ---\n",
    "try:\n",
    "    repo = git.Repo(search_parent_directories=True)\n",
    "    git_commit_hash = repo.head.object.hexsha\n",
    "except Exception as e:\n",
    "    git_commit_hash = \"N/A\" # –ù–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ —Å–∫—Ä–∏–ø—Ç –∑–∞–ø—É—â–µ–Ω –Ω–µ –∏–∑ Git-—Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è\n",
    "    print(f\"Warning: Could not get git commit hash. {e}\")\n",
    "\n",
    "print(f\"Current Git Commit Hash: {git_commit_hash}\")\n",
    "\n",
    "# --- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å ---\n",
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ —Å–∫—Ä–∏–ø—Ç–∞ –Ω–∞—Ä–µ–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö (sample_creator)\n",
    "data_params = {\n",
    "    \"window_size\": 50,\n",
    "    \"step\": 1,\n",
    "    \"sampling_rate\": 5\n",
    "}\n",
    "\n",
    "# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏\n",
    "model_params = {\n",
    "    \"epochs\": 7,\n",
    "    \"batch_size\": 128,\n",
    "    \"validation_split\": 0.2,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss\": \"mean_squared_error\",\n",
    "    \"lr\": 0.002,\n",
    "    \"hidden_dim_1\": 32,\n",
    "    \"hidden_dim_2\": 16,\n",
    "    \"dropout\": 0.2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c208c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/processed/Unit20_win50_str1_smp1.npz', '../data/processed/Unit18_win50_str1_smp1.npz', '../data/processed/Unit5_win50_str1_smp1.npz', '../data/processed/Unit16_win50_str1_smp1.npz', '../data/processed/Unit2_win50_str1_smp1.npz', '../data/processed/Unit10_win50_str1_smp1.npz']\n"
     ]
    }
   ],
   "source": [
    "def load_and_merge_data(npz_units):\n",
    "      sample_array_lst = []\n",
    "      label_array_lst = []\n",
    "      for npz_unit in npz_units:\n",
    "        loaded = np.load(npz_unit)\n",
    "        sample_array_lst.append(loaded['sample'])\n",
    "        label_array_lst.append(loaded['label'])\n",
    "      sample_array = np.dstack(sample_array_lst)\n",
    "      label_array = np.concatenate(label_array_lst)\n",
    "      sample_array = sample_array.transpose(2, 0, 1)\n",
    "      return sample_array, label_array\n",
    "\n",
    "processed_dir = '../data/processed/'\n",
    "\n",
    "# –°–æ–±–∏—Ä–∞–µ–º –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º –¥–ª—è train –∏ test\n",
    "train_files = [os.path.join(processed_dir, f) for f in os.listdir(processed_dir) if f.startswith(('Unit2_', 'Unit5_', 'Unit10_', 'Unit16_', 'Unit18_', 'Unit20_'))]\n",
    "test_files = [os.path.join(processed_dir, f) for f in os.listdir(processed_dir) if f.startswith(('Unit11_', 'Unit14_', 'Unit15_'))]\n",
    "print(train_files)\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "X_train, y_train = load_and_merge_data(train_files)\n",
    "X_test, y_test = load_and_merge_data(test_files)\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º—É –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ X_train\n",
    "n_timesteps, n_features = X_train.shape[1], X_train.shape[2]\n",
    "\n",
    "print('–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏ (X):', X_train.shape)\n",
    "print('–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏ (y):', y_train.shape)\n",
    "print('–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ (X):', X_test.shape)\n",
    "print('–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ (y):', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ab700d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MLflow run...\n",
      "Parameters logged.\n",
      "Epoch 1/5 | Train Loss: 663.87 | Val MAE: 19.18 | PHM08: 8.11\n",
      "Epoch 2/5 | Train Loss: 518.21 | Val MAE: 19.18 | PHM08: 8.13\n",
      "Epoch 3/5 | Train Loss: 516.46 | Val MAE: 19.18 | PHM08: 8.10\n",
      "Epoch 4/5 | Train Loss: 151.47 | Val MAE: 5.45 | PHM08: 0.88\n",
      "Epoch 5/5 | Train Loss: 75.99 | Val MAE: 4.54 | PHM08: 0.63\n",
      "\n",
      "Evaluating on Test Set...\n",
      "Test Metrics: {'mae': 4.3306284, 'rmse': 6.0068874, 'mape': 15.847097337245941, 'phm08_score': 0.6585267404393621}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/21 12:49:52 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\u001b[31m2025/12/21 12:49:56 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Complete. Artifacts logged.\n",
      "üèÉ View run clean-sow-59 at: http://213.21.252.250:5000/#/experiments/2/runs/9f000f382a8640cab5a8b98bc708448d\n",
      "üß™ View experiment at: http://213.21.252.250:5000/#/experiments/2\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    print(\"Starting MLflow run...\")\n",
    "\n",
    "    # --- –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ---\n",
    "    mlflow.log_params(data_params)\n",
    "    mlflow.log_params(model_params)\n",
    "    mlflow.set_tag(\"git_commit\", git_commit_hash)\n",
    "    print(\"Parameters logged.\")\n",
    "\n",
    "    # --- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è PyTorch ---\n",
    "    # 1. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º numpy –º–∞—Å—Å–∏–≤—ã –≤ torch —Ç–µ–Ω–∑–æ—Ä—ã\n",
    "    X_train_tensor = torch.from_numpy(X_train).float()\n",
    "    y_train_tensor = torch.from_numpy(y_train).float().view(-1, 1) # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ —Ñ–æ—Ä–º–∞ (batch_size, 1)\n",
    "    X_test_tensor = torch.from_numpy(X_test).float()\n",
    "    y_test_tensor = torch.from_numpy(y_test).float().view(-1, 1)\n",
    "\n",
    "    # 2. –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    # 3. –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫–∏ –≤—Ä—É—á–Ω—É—é\n",
    "    val_split = model_params['validation_split']\n",
    "    dataset_size = len(train_dataset)\n",
    "    val_size = int(val_split * dataset_size)\n",
    "    train_size = dataset_size - val_size\n",
    "    train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "    # 4. –°–æ–∑–¥–∞–µ–º –∑–∞–≥—Ä—É–∑—á–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö (DataLoader), –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –ø–æ–¥–∞–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –±–∞—Ç—á–∞–º–∏\n",
    "    train_loader = DataLoader(dataset=train_subset, batch_size=model_params['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(dataset=val_subset, batch_size=model_params['batch_size'])\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=model_params['batch_size'])\n",
    "\n",
    "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏\n",
    "    device = torch.device(\"cpu\") # –¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ: —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ CPU\n",
    "    \n",
    "    model = AdaptiveLSTMModel(\n",
    "        input_dim=n_features,\n",
    "        hidden_dim_1=model_params[\"hidden_dim_1\"],\n",
    "        hidden_dim_2=model_params[\"hidden_dim_2\"],\n",
    "        dropout_prob=model_params[\"dropout\"]\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=model_params[\"lr\"])\n",
    "\n",
    "    # –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è\n",
    "    for epoch in range(model_params[\"epochs\"]):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        mlflow.log_metric(\"train_mse_loss\", avg_train_loss, step=epoch)\n",
    "\n",
    "        # –í–∞–ª–∏–¥–∞—Ü–∏—è\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_targets = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                val_preds.append(outputs.numpy())\n",
    "                val_targets.append(labels.numpy())\n",
    "        \n",
    "        val_preds = np.concatenate(val_preds)\n",
    "        val_targets = np.concatenate(val_targets)\n",
    "        \n",
    "        # –†–∞—Å—á–µ—Ç –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫\n",
    "        val_metrics = calculate_metrics(val_targets, val_preds)\n",
    "        \n",
    "        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "        for name, value in val_metrics.items():\n",
    "            mlflow.log_metric(f\"val_{name}\", value, step=epoch)\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}/{model_params['epochs']} | Train Loss: {avg_train_loss:.2f} | Val MAE: {val_metrics['mae']:.2f} | PHM08: {val_metrics['phm08_score']:.2f}\")\n",
    "\n",
    "    # --- 5. –§–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ---\n",
    "    print(\"\\nEvaluating on Test Set...\")\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    test_targets = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            test_preds.append(outputs.numpy())\n",
    "            test_targets.append(labels.numpy())\n",
    "    \n",
    "    test_preds = np.concatenate(test_preds)\n",
    "    test_targets = np.concatenate(test_targets)\n",
    "    \n",
    "    test_metrics = calculate_metrics(test_targets, test_preds)\n",
    "    print(f\"Test Metrics: {test_metrics}\")\n",
    "    \n",
    "    # –õ–æ–≥–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º test_\n",
    "    for name, value in test_metrics.items():\n",
    "        mlflow.log_metric(f\"test_{name}\", value)\n",
    "\n",
    "    # 1. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ü–û–õ–ù–û–ô –º–æ–¥–µ–ª–∏\n",
    "    mlflow.pytorch.log_model(model, \"full_model\")\n",
    "    \n",
    "    # 2. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ BACKBONE (State Dict –±–µ–∑ –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è) –¥–ª—è Transfer Learning\n",
    "    # –ò—Å–∫–ª—é—á–∞–µ–º –≤–µ—Å–∞ input_adapter, —á—Ç–æ–±—ã –∫–ª–∏–µ–Ω—Ç –º–æ–≥ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ–∏\n",
    "    backbone_state_dict = {k: v for k, v in model.state_dict().items() if \"input_adapter\" not in k}\n",
    "    torch.save(backbone_state_dict, \"lstm_backbone.pth\")\n",
    "    mlflow.log_artifact(\"lstm_backbone.pth\", artifact_path=\"transfer_learning_artifacts\")\n",
    "    \n",
    "    print(\"Run Complete. Artifacts logged.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
